Je vais préparer un guide détaillé sur le contrôle et l'amélioration des performances système sous Debian Linux. Il couvrira les aspects suivants :
- Recherche des problèmes de performance : identification des goulets d'étranglement et des limitations du système.
- Analyse des différentes couches : CPU, RAM, disque, processus, et interactions réseau.
- Tests de performance : outils classiques (htop, iostat, etc.) et outils plus avancés.
- Méthodes et solutions pour optimiser les performances.
- Introduction à la supervision centralisée avec Prometheus.

Je vous tiens informé dès que le guide est prêt.

# Contrôle et amélioration des performances système sous Debian Linux

**Introduction :** La stabilité et la réactivité d’un système Debian dépendent d’une surveillance proactive de ses ressources. En production, la **surveillance des performances** doit être un processus continu afin de garder un œil sur le CPU, la mémoire, le disque et le réseau du serveur ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=La%20surveillance%20des%20performances%20des,pour%20%C3%A9viter%20les%20goulots%20d%E2%80%99%C3%A9tranglement)). Ce guide détaille les méthodes pour identifier des **goulets d’étranglement**, analyser en profondeur chaque couche du système, tester les performances, optimiser les paramètres et mettre en place une **supervision centralisée** avec Prometheus et Grafana.

## 1. Recherche des problèmes de performance

La première étape consiste à **identifier les ressources saturées** qui limitent les performances. Un goulot d’étranglement peut provenir d’un CPU surchargé, d’une mémoire insuffisante, d’un disque trop sollicité ou d’un réseau saturé. En surveillant l’utilisation de chaque ressource, on détermine celle qui contraint le système (par exemple, une utilisation CPU à 100%, une mémoire saturée entraînant du swap, un %IOwait élevé, etc.). Dès qu’une ressource est identifiée comme tendue, on peut explorer plus finement sa cause.

- **Utilisation CPU :** La commande `top` ou son pendant amélioré `htop` fournit en temps réel la charge du processeur, l’utilisation mémoire et la liste des processus ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=,utilisateur%20et%20des%20fonctionnalit%C3%A9s%20suppl%C3%A9mentaires)). On y observe la **charge moyenne** (*load average*) du système sur 1, 5 et 15 minutes, qui indique le nombre de processus actifs ou en attente du CPU ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=Il%20est%20important%20de%20consid%C3%A9rer,pour%20%C3%A9valuer%20les%20informations)). Une charge constamment supérieure au nombre de cœurs du CPU signale une surcharge processeur. `top/htop` montrent aussi les processus les plus gourmands en CPU (colonne `%CPU`) et le taux d’**utilisation CPU** global (utilisateur, système, idle, iowait) ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=T%C3%A2ches%3A%20500%20total%2C%20%20,arr%C3%AAt%C3%A9%2C%20%20%200%20zombie)). Par exemple, un pourcentage `%iowait` élevé indique que le CPU attend souvent le disque, pointant vers un éventuel problème d’IO ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=Un%20pourcentage%20%C3%A9lev%C3%A9%20d%E2%80%99,Voici%20pourquoi)). Pour une vue détaillée par cœur ou déceler des interruptions matérielles excessives, on peut utiliser `mpstat -P ALL` (du paquet `sysstat`) qui donne l’usage CPU par cœur et le nombre d’interruptions par seconde.

- **Utilisation de la RAM :** La commande `free -m` offre un aperçu rapide de la mémoire totale, utilisée, libre, et du swap ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=Pour%20surveiller%20l%E2%80%99utilisation%20de%20la,commandes%20peuvent%20%C3%AAtre%20utilis%C3%A9es)). Si la mémoire libre est très basse et que la partie *swap* est utilisée de manière significative, le système peut passer du temps à échanger des données sur le disque (phénomène de **swapping**). L’outil `vmstat 1 5` affiche un résumé sur quelques secondes de l’activité système (processus en attente, mémoire libre, swap, IO) qui permet de repérer un manque de RAM ou une activité de swap importante ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=procs%20,cpu)). Une utilisation massive du swap s’accompagne souvent d’une augmentation de la latence et d’une baisse de performance. Il convient alors d’identifier les processus consommateurs de mémoire (par exemple via `top` trié par `%MEM`) et de vérifier s’il n’y a pas de **fuites de mémoire** (processus dont l’usage mémoire augmente continuellement). Des outils spécialisés comme `smem` ou `pmap` peuvent aider à détailler la consommation mémoire par processus si besoin.

- **Entrées/Sorties disque :** Les performances disque peuvent être un facteur limitant lorsque les opérations d’IO sont intensives. La commande `iotop` permet de lister en temps réel les processus les plus gourmands en lecture/écriture disque ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=,les%20plus%20gourmands%20en%20IO)). Pour une vue globale, `iostat -x 1 5` (fourni par le paquet `sysstat`) affiche les statistiques par périphérique de disque, notamment le débit (kB/s) et le taux d’**utilisation (%util)** du disque ([Cinq commandes Linux pour monitorer les performances des serveurs | LeMagIT](https://www.lemagit.fr/conseil/Cinq-commandes-Linux-pour-monitorer-les-performances-des-serveurs#:~:text=Le%20second%20rapport%20montre%20chaque,m%2C%20respectivement)). Un pourcentage d’utilisation proche de 100% indique que le disque est saturé en permanence. De même, la colonne « await » de `iostat -x` révèle la **latence moyenne** des opérations (temps d’attente). Des valeurs de latence élevées ou un long temps dans la file d’attente (*avgqu-sz*) suggèrent un goulet d’étranglement au niveau stockage. L’outil `vmstat` mentionné plus haut donne également le nombre de blocs lus/écrits par seconde (colonnes `bi`/`bo`) et peut indiquer une activité disque soutenue ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=procs%20,cpu)). En complément, `df -h` permet de vérifier que le système n’est pas à court d’espace disque, ce qui pourrait dégrader les performances (un disque plein peut entraîner des écritures très lentes).

- **Trafic réseau :** Une surcharge réseau peut ralentir les applications communicantes. On utilisera `sar -n DEV 1 5` (du paquet `sysstat`) ou des outils spécialisés comme `iftop`, `nload` ou `bmon` pour surveiller le débit entrant/sortant sur les interfaces. Par exemple, `iftop -i eth0` affiche en temps réel le **débit réseau** sur l’interface `eth0` et les hôtes distants les plus consommateurs ([Linux Commands to Troubleshoot Performance Issues - DevOps Freelancer](https://www.devopsfreelancer.com/blog/linux-commands-to-troubleshoot-performance-issues/#:~:text=23)). Des débits constamment au maximum de la capacité de l’interface (par ex. ~100% de 1 Gbps) révèlent un goulet d’étranglement réseau. La commande `ss -tuna` peut également lister les connexions TCP/UDP et leurs états pour voir si de nombreuses connexions saturent la machine. En cas de latences réseau élevées, il faut évaluer la bande passante disponible, la perte de paquets ou la saturation de la pile réseau (par exemple en examinant les compteurs d’erreurs avec `netstat -i` ou `ip -s link`).

- **Outils avancés et alternatifs :** Outre les utilitaires standard, il existe des outils récents facilitant le diagnostic. **Glances** est un moniteur en mode texte qui affiche sur une seule interface l’essentiel des métriques (CPU, mémoire, disque, réseau, processus) et s’adapte dynamiquement à la taille du terminal ([How to Use Glances for System Monitoring on Linux |
Linode Docs](https://www.linode.com/docs/guides/how-to-use-glances-system-monitoring/#:~:text=Glances%20is%20a%20Linux%20system,to%20the%20current%20screen%20size)) ([Real-Time Linux Monitoring with dstat: A Powerful Tool for SREs and Admins | by Ramkrushna Maheshwar | Feb, 2025 | Medium](https://medium.com/@maheshwar.ramkrushna/real-time-linux-monitoring-with-dstat-a-powerful-tool-for-sres-and-admins-37012c65ff51#:~:text=%60dstat%60%20is%20a%20real,to%20analyze%20the%20system%E2%80%99s%20performance)). De même, **atop** est un outil interactif similaire à top, qui enregistre en outre des historiques pour analyse a posteriori et fournit un niveau de détail très fin sur toutes les ressources critiques du système (y compris l’utilisation détaillée par processus) ([ATOP – Au coeur de la supervision de performance. – Linux French](https://linuxfrench.wordpress.com/2017/06/21/atop-au-coeur-de-la-supervision-de-performance/#:~:text=ATOP%20est%20un%20outil%20de,nous%20int%C3%A9ressons%20aussi%20dans%20cet)). Ces outils polyvalents évitent de lancer séparément `top`, `iostat`, `vmstat`, etc., et peuvent enregistrer des **logs de performance** pour comprendre un incident passé. Enfin, pour des analyses plus pointues, on peut employer des outils de profilage et de trace : par exemple `perf` (perf_events) pour profiler l’utilisation CPU, identifier des *hotspots* ou mesurer des événements matériels, et `strace` pour suivre les appels système d’un processus lent. En traçant un processus avec `strace -p <PID> -T -e trace=%cpu%` (ou autre filtre), on peut repérer s’il est bloqué sur une opération système particulière (lecture disque, accès réseau, attente de verrou, etc.). L’utilisation de `strace` sur un processus permet ainsi d’**identifier des inefficacités ou des blocages au niveau des appels système**, contribuant à des problèmes de performance ([Effective Annotating of Strace Output for MySQL Performance Troubleshooting](https://www.linkedin.com/pulse/effective-annotating-strace-output-mysql-performance-shiv-iyer-iyvkc#:~:text=Linux%20Process%20Snapper%20,handling%20of%20concurrent%20read%20operations)). Il convient toutefois d’utiliser ces outils avancés avec précaution en production (ils ajoutent une charge d’observation) et plutôt en phase de diagnostic ciblé.

 ([htop - an interactive process viewer](https://htop.dev/screenshots.html)) *Exemple de vue en temps réel des processus et de l’utilisation des ressources avec **htop**. Chaque colonne affiche des métriques par processus (CPU%, MEM%, temps CPU, commande…), tandis que le bandeau supérieur montre la charge moyenne, l’utilisation globale CPU par cœur, la mémoire et le swap disponibles.* ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=,utilisateur%20et%20des%20fonctionnalit%C3%A9s%20suppl%C3%A9mentaires))

En résumé, **inspecter régulièrement le CPU, la RAM, le disque et le réseau** avec ces outils permet de **déceler rapidement le composant limitant** les performances du système. Une fois le goulot d’étranglement identifié, on peut passer à l’analyse plus fine de la couche correspondante.

## 2. Analyse des différentes couches

Après avoir cerné la ressource problématique, on approfondit l’analyse au niveau de chaque couche du système : CPU, mémoire, I/O disque et processus. L’objectif est de comprendre **pourquoi** une ressource est saturée et comment y remédier. 

### Analyse des performances CPU

Lorsque le processeur semble surchargé, on examine d’abord la **charge moyenne (load average)** et l’utilisation détaillée du CPU. Le *load average* fournit la moyenne du nombre de tâches en cours d’exécution ou en attente par le CPU ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=Le%20%E2%80%9Cload%20average%E2%80%9D%20est%20un,CPU)). Il est présenté sous trois valeurs (1, 5 et 15 minutes) et doit être interprété en fonction du nombre de cœurs : par exemple, un load de *8.0* sur un système 4 cœurs indique 2 fois plus de demandes que la capacité, donc un CPU engorgé ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=Il%20est%20important%20de%20consid%C3%A9rer,pour%20%C3%A9valuer%20les%20informations)). Ensuite, la décomposition du temps CPU est cruciale. Les états **user**, **system**, **idle**, **iowait** (et éventuellement **steal** en environnement virtuel) décrivent comment le CPU passe son temps ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=1.%20CPU%20idle%20%28%60,inactif%20et%20n%E2%80%99ex%C3%A9cute%20aucune%20t%C3%A2che)) ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=4.%20Attente%20d%E2%80%99IO%20%28%60,se%20terminent)). Un CPU constamment à 90-100% en temps *user* ou *system* signifie qu’il exécute en permanence des tâches intensives. Un `%iowait` élevé indique que le CPU est souvent inactif en attente d’IO disque ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=4.%20Attente%20d%E2%80%99IO%20%28%60,se%20terminent)), signe d’un ralentissement causé par le stockage. On peut utiliser `pidstat -u 1` pour voir l’usage CPU par processus à intervalle régulier, ou `top -H` pour visualiser l’utilisation par thread et repérer une éventuelle **contention** (ex: threads en attente active). En cas de charge CPU élevée, il faut identifier si quelques processus monopolisent le CPU (p. ex. un calcul intensif, un processus coincé dans une boucle, etc.) ou si c’est une somme de nombreuses tâches. Les **interruptions** matérielles excessives peuvent aussi peser sur le CPU : la commande `vmstat` affiche le nombre d’interruptions (`in`) et de commutations de contexte (`cs`) par seconde ([Cinq commandes Linux pour monitorer les performances des serveurs | LeMagIT](https://www.lemagit.fr/conseil/Cinq-commandes-Linux-pour-monitorer-les-performances-des-serveurs#:~:text=envoy%C3%A9s%20disque)). Des valeurs exceptionnellement hautes pourraient indiquer un périphérique posant problème (interruptions fréquentes) ou un noyau submergé par le traitement d’IRQ. Outils utiles à ce stade : `perf top` peut révéler en temps réel quelles fonctions (du kernel ou d’un processus) consomment du CPU, et `mpstat -I CPU` (sysstat) détaillera les interruptions par processeur. Une **contention CPU** (trop de processus pour le temps CPU disponible) se manifeste par un load average élevé et de nombreux processus dans la file d’exécution (colonne `r` de `vmstat` ou de `uptime`). Si le CPU est identifié comme le goulot d’étranglement, on envisagera des optimisations comme le réglage d’affinité CPU (pinning de certains processus sur des cœurs dédiés via `taskset`), la désactivation de fonctions consommatrices si possible, ou l’upgrade matériel (CPU plus rapide ou plus de cœurs) en dernier recours.

### Analyse de la mémoire

Une **mémoire saturée** peut provoquer un ralentissement généralisé du système. Pour évaluer l’état de la RAM, on examine la quantité de mémoire libre et utilisée, le swap et l’utilisation des caches. La commande `free -h` indique la mémoire *disponible* en tenant compte des buffers et cache (car Linux utilise la RAM inutilisée pour du cache disque, libérable si nécessaire). Si la mémoire disponible est quasi nulle et que le swap est fortement sollicité, le système passe du temps à transférer des pages mémoire sur le disque, ce qui se traduit par des accès disque accrus et des performances en chute. On parle de **contention mémoire** lorsque la demande dépasse la RAM physique ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=Contention%20M%C3%A9moire)). Des symptômes typiques incluent une activité swap élevée (`si/so` dans `vmstat` non nuls régulièrement) et un temps d’accès aux applications qui augmente. Pour diagnostiquer, on peut utiliser `pidstat -r 1` afin de surveiller l’utilisation mémoire de chaque processus en temps réel, ou `smem` pour un rapport synthétique. Repérer un processus en particulier consommant une part énorme de RAM (potentielle fuite mémoire) est essentiel. La présence de processus en état *OOM killer* dans les logs (`dmesg` ou `/var/log/kern.log`) signale que le noyau a dû tuer des applications faute de mémoire ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=L%E2%80%99%60OOM,pour%20r%C3%A9soudre%20cette%20situation%20critique)) ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=Il%20est%20important%20de%20noter,les%20ressources%20mat%C3%A9rielles%20si%20n%C3%A9cessaire)). On vérifiera aussi le ratio de cache et buffers : Linux alloue souvent beaucoup de mémoire au cache disque (visible dans `free`) mais cela n’est pas problématique tant que ce cache peut être libéré pour les applications actives (la colonne “available” de `free` en tient compte). En revanche, si même les caches sont minimes, la mémoire est réellement saturée. Une autre métrique utile est le taux de **page faults** (majeurs) via `sar -B` ou `vmstat`, qui augmente fortement en cas de swaps fréquents. Enfin, pour détecter d’éventuelles *memory leaks*, on peut observer sur la durée l’évolution de la mémoire occupée par un processus suspect (par exemple avec `pmap <PID>` ou en monitorant sa RSS via `pidstat`). Si un processus voit sa mémoire croître sans jamais diminuer, il peut nécessiter un redémarrage ou une correction applicative.

### Analyse des entrées/sorties disque

La **performance du stockage** se mesure en termes de débit (vitesse de lecture/écriture), de latence (temps d’accès) et de concurrence (taille des files d’attente d’IO). Un disque traditionnel (HDD) saturé aura des temps d’accès bien plus élevés qu’un SSD, et un serveur peut montrer des signes de ralentissement important si les disques n’arrivent plus à suivre la charge. Pour analyser ce niveau, on utilise `iostat -x` qui indique pour chaque disque le **débit** (kB lu/écrit par sec), le temps moyen d’attente (**await**) et le pourcentage d’**utilisation** du disque ([Cinq commandes Linux pour monitorer les performances des serveurs | LeMagIT](https://www.lemagit.fr/conseil/Cinq-commandes-Linux-pour-monitorer-les-performances-des-serveurs#:~:text=Le%20second%20rapport%20montre%20chaque,m%2C%20respectivement)). Un await élevé (par ex > 20 ms de moyenne sur un SSD, ou > 5-10 ms sur un NVMe) suggère que les opérations mettent du temps à se compléter, possiblement à cause d’une file d’attente trop longue. La colonne `%util` proche de 100% sur un disque indique qu’il est sollicité en permanence sans temps idle, ce qui est un signe de saturation. Le nombre moyen de requêtes en file (`avgqu-sz`) visible avec `iostat -x` donne aussi une idée de la **profondeur de queue** : si beaucoup d’opérations s’accumulent en attente, la queue s’allonge. En parallèle, `iotop` aide à repérer quel processus génère le plus d’IO en temps réel, ce qui est utile pour isoler un service particulier (par exemple une base de données ou un processus de sauvegarde) responsable de la charge disque ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=,threads%20qui%20en%20sont%20responsables)) ([Optimisation des Performances d'un Serveur Linux | DevSecOps](https://blog.stephane-robert.info/docs/admin-serveurs/linux/performances/#:~:text=TID%20%20PRIO%20%20USER,COMMAND)). On pourra également examiner les **comptoirs d’IO du noyau** via `/proc/diskstats` ou `sar -d` pour avoir des statistiques cumulées (nombre total de lectures/écritures, temps passé en IO, etc.). Des outils de benchmarking comme `hdparm -t` ou `dd` en lecture séquentielle peuvent donner une référence du débit maximal du disque, à comparer à ce que l’on observe en production. Si le disque est identifié comme lent, il faut vérifier son état (pas d’erreurs SMART), la politique d’**ordonnancement d’IO** (scheduler *cfq*, *deadline*, *noop* – un scheduler inadapté peut nuire aux performances selon la charge, par ex. on privilégie souvent *deadline* ou *noop* pour les SSD). De plus, la configuration du RAID ou du LVM peut impacter la latence : un RAID logiciel en reconstruction ou un volume chiffré peuvent introduire des délais. En résumé, une analyse fine des IO consiste à vérifier que le **temps d’accès moyen reste faible** et que le débit est en adéquation avec le matériel. Si ce n’est pas le cas, on envisagera soit d’alléger la charge IO (mise en cache applicative, étalement des traitements), soit d’améliorer le stockage (disque plus rapide, augmentation du nombre de disques en parallèle, etc.).

### Surveillance des processus et threads

Au-delà des ressources brutes, il est important d’analyser l’état des **processus** du système, car des situations anormales peuvent dégrader les performances. Par exemple, des **processus zombies** (états *defunct*) sont des processus terminés dont le parent n’a pas lu le code de sortie – ils ne consomment pas de ressources significatives mais peuvent indiquer des problèmes de gestion de processus dans les applications. On les repère avec `ps aux | grep Z` ou dans `top` (état “zombie”). Plus préoccupants sont les processus bloqués en attente d’IO (*D* uninterruptible sleep) : un processus coincé en état *D* indique qu’il attend une ressource (généralement IO disque/NFS) sans pouvoir être interrompu. De nombreux processus en état *D* traduisent souvent un **goulet d’étranglement disque ou réseau** (ex: attente de réponses d’un stockage NFS lent). La commande `ps -eo state,pid,comm | grep "^D"` permet de lister les tâches bloquées. En cas de blocage, on peut attacher un `strace` pour voir sur quelle opération le processus est figé (lecture de fichier, appel réseau...). On peut aussi utiliser `pidstat -p <PID> -d 1` pour surveiller l’IO d’un processus spécifique et voir s’il progresse. Par ailleurs, on surveillera la **contention de ressources** au niveau applicatif : par exemple des threads en grand nombre en attente d’un verrou (verrous utilisateur ou verrous kernel). Des outils de profilage de verrous comme `perf lock` ou `sysdig`/`bcc` scripts (ex: `deadlock_detector`) peuvent aider, mais souvent un symptôme visible est un processus occupant du CPU sans avancer (boucle d’attente active) ou au contraire inactif car bloqué. Si une application multi-thread est ralentie, `htop` en mode affichage des threads aide à voir si tous les threads sont actifs ou si certains sont bloqués. On peut repérer la **charge par thread** (via `htop` ou `pidstat -t`) pour équilibrer le travail. Enfin, vérifier le nombre total de processus et threads du système (`ps -eLf | wc -l`) peut prévenir une saturation : un fork bomb ou un service incontrôlé spawnant trop de processus peut épuiser le CPU et la mémoire rapidement. L’analyse des processus passe donc par l’identification de tout comportement anormal (zombies persistants, threads bloqués, exécution concurrente excessive) afin d’isoler les problèmes logiciels pouvant être à l’origine des baisses de performance.

## 3. Tester les performances du système

Une fois le diagnostic en cours, il peut être utile de réaliser des **tests de performance ciblés** pour mesurer les capacités du système et observer son comportement sous charge. Ces tests de benchmarking permettent de quantifier le débit du CPU, de la mémoire ou du disque, et de valider les optimisations. Ils peuvent aussi servir à reproduire un problème dans un environnement contrôlé.

- **Benchmarks CPU, RAM et disque :** Des outils comme **sysbench** ou **stress-ng** permettent de solliciter de façon artificielle les composants du système. Par exemple, `sysbench --test=cpu --cpu-max-prime=20000 run` exécute un calcul intensif (recherche de nombres premiers) pour mesurer le **temps de calcul CPU**. De même, `sysbench --test=memory run` évalue le débit mémoire (lecture/écriture) en RAM, et le mode `fileio` de sysbench sert à générer des I/O disque (lecture/écriture aléatoire ou séquentielle) sur un fichier de test. Sysbench est multi-threadé et permet donc de tester l’effet de plusieurs threads en parallèle sur le CPU ou le disque. **stress-ng** est un autre outil très complet qui propose des *stressors* pour tous les sous-systèmes (CPU: calculs mathématiques, mémoire: allocations répétées, I/O disque: écritures continues, etc.) ([Chapter 29. Stress testing real-time systems with stress-ng | Red Hat ...](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/8/html/optimizing_rhel_8_for_real_time_for_low_latency_operation/assembly_stress-testing-real-time-systems-with-stress-ng_optimizing-rhel8-for-real-time-for-low-latency-operation#:~:text=Chapter%2029.%20Stress%20testing%20real,stress%20mechanisms%20known%20as%20stressors)). Par exemple, `stress-ng --cpu 4 --vm 2 --vm-bytes 1G --timeout 60s` va charger 4 cœurs CPU, allouer et utiliser 2 segments de 1 Go de RAM (test mémoire) pendant 60 secondes. L’intérêt de ces benchmarks synthétiques est de vérifier la stabilité et la performance maximale du matériel : par exemple, s’assurer que le CPU ne surchauffe pas (throttling), ou que le débit disque atteint les valeurs attendues. Ils permettent aussi de **calibrer le monitoring** : en lançant un stress test, on peut observer avec `top`, `iostat`, etc., la réponse du système (par ex. voir la montée en charge, la température si on a des capteurs, l’activation éventuelle du swap, etc.).

- **Tests de performance applicatifs :** Outre les benchmarks synthétiques, il est souvent pertinent de tester les applications elles-mêmes. Par exemple, pour un serveur web, on utilisera des outils de charge comme `ab` (ApacheBench), `wrk` ou `siege` pour générer du trafic HTTP et mesurer le nombre de requêtes par seconde que le serveur peut traiter. Pour une base de données, on peut utiliser `sysbench --test=oltp` (pour MySQL) ou `pgbench` (pour PostgreSQL) afin de simuler des transactions. L’objectif est de reproduire une charge proche de la production et de voir comment l’application et le système se comportent (utilisation CPU, latence de réponse, éventuels erreurs ou timeouts). Pendant ces tests, il est recommandé de **surveiller en temps réel** les métriques du système pour identifier d’éventuels nouveaux goulets d’étranglement apparaissant sous forte charge. Des outils tels que `dstat` ou `sar` sont très utiles dans ce contexte. Par exemple, on peut lancer `dstat -tcndylam` pendant un test de charge : cela affichera chaque seconde l’utilisation CPU, réseau, disque, la mémoire libre, etc., sur une seule ligne, ce qui facilite le suivi de l’impact du test sur l’ensemble du système ([Real-Time Linux Monitoring with dstat: A Powerful Tool for SREs and Admins | by Ramkrushna Maheshwar | Feb, 2025 | Medium](https://medium.com/@maheshwar.ramkrushna/real-time-linux-monitoring-with-dstat-a-powerful-tool-for-sres-and-admins-37012c65ff51#:~:text=%60dstat%60%20is%20a%20real,to%20analyze%20the%20system%E2%80%99s%20performance)) ([Real-Time Linux Monitoring with dstat: A Powerful Tool for SREs and Admins | by Ramkrushna Maheshwar | Feb, 2025 | Medium](https://medium.com/@maheshwar.ramkrushna/real-time-linux-monitoring-with-dstat-a-powerful-tool-for-sres-and-admins-37012c65ff51#:~:text=%2A%20Real,to%20CSV%20for%20further%20analysis)). De même, `pidstat -h -u -p <PID> 5` peut suivre l’utilisation CPU d’un processus particulier (par exemple le serveur web) toutes les 5 secondes pendant le test, pour voir comment son utilisation CPU évolue.

- **Monitoring en temps réel lors des tests :** Le paquet **sysstat** offre la commande `sar` qui peut enregistrer dans des fichiers historiques l’activité du système à intervalles réguliers. On peut programmer `sar` pour qu’il capture les métriques chaque minute et ainsi avoir un historique durant un test de charge. L’avantage est de pouvoir ensuite analyser tranquillement les données (CPU, mémoire, I/O, réseau) sur la période du test, voire de les grapher. `sar -u` donnera l’historique CPU, `sar -r` la mémoire, `sar -b` les transferts disque, etc. ([Cinq commandes Linux pour monitorer les performances des serveurs | LeMagIT](https://www.lemagit.fr/conseil/Cinq-commandes-Linux-pour-monitorer-les-performances-des-serveurs#:~:text=La%20commande%20sar%20permet%20%C3%A9galement,le%20r%C3%A9seau%20et%20les%20blocs)) ([Cinq commandes Linux pour monitorer les performances des serveurs | LeMagIT](https://www.lemagit.fr/conseil/Cinq-commandes-Linux-pour-monitorer-les-performances-des-serveurs#:~:text=Pour%20sp%C3%A9cifier%20les%20donn%C3%A9es%20%C3%A0,pour%20voir%20l%E2%80%99ensemble%20des%20m%C3%A9triques)). Alternativement, l’outil **Grafana** (voir section 5) peut être utilisé pendant un test pour visualiser en direct les métriques. L’important est d’utiliser ces outils de **monitoring en conjonction avec les tests** afin de lier les chiffres bruts de performance (ex: X requêtes/s) avec l’état du système (ex: CPU à Y%, latence disque de Z ms). Cela permet d’identifier précisément la ressource saturée lors du test. Par exemple, si un test de base de données montre une chute du nombre de transactions au-delà d’un certain seuil, les mesures en parallèle pourraient révéler que c’est dû à la saturation du CPU ou du disque, orientant ainsi les optimisations.

- **Analyse par cgroups (contrôle des ressources) :** Sur Debian (avec systemd), on peut tirer parti des **cgroups** pour mesurer et limiter la consommation des ressources par groupe de processus. L’outil `systemd-cgtop` affiche en temps réel la consommation CPU et mémoire par groupe de contrôle (souvent par service systemd) – on y voit par exemple si un conteneur ou un service particulier utilise une part disproportionnée des ressources. Durant un test de charge, `systemd-cgtop` aide à vérifier que les ressources sont bien réparties entre les services attendus. On peut également créer des cgroups spécifiques pour un processus de test afin d’isoler son impact. Par exemple, on peut lancer un benchmark dans un cgroup mémoire limité pour voir comment il se comporte avec moins de RAM. Les cgroups permettent aussi de **limiter** lors des tests pour simuler un environnement contraint (ex: brider un processus à 50% CPU pour voir s’il tient la charge). Debian fournit la commande `cgcreate` et `cgexec` (dans le paquet `cgroup-tools`) ou simplement des directives systemd pour assigner des limites. Cette approche est plus avancée, mais `systemd-cgtop` en lecture seule est déjà utile pour identifier quel groupe de processus consomme quoi, sans avoir à éplucher processus par processus.

En somme, les tests de performance – qu’ils soient généraux avec sysbench/stress-ng ou spécifiques à une application – permettent de **valider les capacités** du système et de déclencher des situations de charge pour mieux les comprendre. Combinés à un monitoring approprié (`sar`, `dstat`, `pidstat`, `cgroups`…), ils donnent une vision claire des **comportements du système en régime de stress**, et donc des pistes d’optimisation à appliquer.

## 4. Identifier et résoudre les goulets d’étranglement

Après avoir identifié un composant limitant, l’étape cruciale est d’**apporter des corrections** pour lever le goulet d’étranglement. Cela passe par de l’optimisation logicielle (paramètres du noyau, configuration système) ou de la gestion des priorités, afin d’utiliser au mieux les ressources existantes. Voici les principales approches pour **tuner** un système Debian et améliorer ses performances :

- **Optimisation des paramètres noyau (sysctl) :** De nombreux comportements du système Linux peuvent être ajustés via des paramètres dans `/etc/sysctl.conf` (ou à la volée avec `sysctl`). Par exemple, on peut réduire l’**agressivité du swapping** en ajustant `vm.swappiness`. Par défaut souvent à 60, une valeur plus basse (e.g. 10) incite le noyau à éviter de swapper tant que possible, gardant les données en RAM ([Understanding vm.swappiness: Improving Linux Performance and Memory Management](https://coderstalk.blogspot.com/2023/07/understanding-vmswappiness-improving.html#:~:text=The%20,to%20a%20different%20swappiness%20behavior)) ([Understanding vm.swappiness: Improving Linux Performance and Memory Management](https://coderstalk.blogspot.com/2023/07/understanding-vmswappiness-improving.html#:~:text=For%20systems%20with%20ample%20RAM%2C,processes%20and%20improving%20overall%20responsiveness)). Cela peut améliorer la réactivité si on a suffisamment de RAM, en évitant d’écrire prématurément en swap. D’autres paramètres mémoire incluent `vm.dirty_ratio` et `vm.dirty_background_ratio` qui contrôlent la taille des caches d’écriture disque : les abaisser peut réduire les pics d’IO en évacuant plus régulièrement les écritures (au prix d’un usage disque plus constant). Pour le réseau, on peut augmenter les tampons noyau (`net.core.rmem_max`, `net.core.wmem_max`) pour optimiser les débits TCP élevés, ou `net.core.somaxconn` si l’on attend de très nombreuses connexions simultanées (file d’attente des sockets TCP en écoute). Au niveau CPU, il y a moins de *sysctl* directs, mais on peut jouer sur l’**ordonnanceur de tâches** via `kernel.sched_*` (par exemple `kernel.sched_min_granularity_ns` pour ajuster la granularité du scheduling CFS). Néanmoins, ces réglages CPU sont délicats et généralement le noyau s’adapte bien tout seul. Une approche plus simple pour le CPU est de vérifier le gouverneur de fréquence : sur un serveur Debian, on peut mettre les CPU en mode performance (plutôt que powersave) pour qu’ils tournent à pleine fréquence en permanence, ce qui réduit la latence au prix d’une consommation électrique un peu supérieure. Ce réglage se fait via `cpupower frequency-set -g performance` ou en utilisant des outils comme **Tuned**. Par exemple, le profil *throughput-performance* de Tuned active le gouverneur *performance* et ajuste certains *sysctl* (comme `vm.dirty_ratio=10`) pour privilégier les performances ([Debian : Optimisez votre distribution avec Tuned - slash-root.fr](https://slash-root.fr/debian-optimisez-votre-distribution-avec-tuned/#:~:text=)). Il est recommandé de toujours tester les changements de sysctl sur un environnement de préproduction, car de mauvais réglages peuvent dégrader la stabilité. Toutefois, bien ajustés, ces paramètres permettent de mieux adapter le système à la charge cible (ex: un serveur DB bénéficiera d’un swapiness faible et de gros tampons disque, un serveur de fichiers peut tirer parti d’une augmentation des buffers réseau, etc.).

- **Réduction de la consommation mémoire excessive :** Si l’analyse a révélé que la mémoire est le facteur limitant, plusieurs mesures peuvent soulager le système. D’abord, agir au niveau applicatif en optimisant la configuration des services (par exemple limiter le nombre de processus Apache ou la taille du cache d’une base de données pour qu’ils rentrent dans la RAM disponible). On peut ensuite ajuster le comportement du noyau vis-à-vis du swap, comme mentionné, via `vm.swappiness`. En complément, Debian permet l’usage de **zRAM** ou **zswap** – des mécanismes de compression de la RAM ou du swap – qui peuvent être bénéfiques si le swap est inévitable : au lieu d’écrire sur le disque, le noyau compresse des pages en mémoire, ce qui allège les IO et peut augmenter la quantité apparente de RAM (au prix d’un peu de CPU pour la compression). En cas de saturation mémoire, surveiller l’**OOM-Killer** est important : si le noyau commence à tuer des processus (événement loggué dans `dmesg`), il faut soit ajouter de la RAM, soit diminuer la charge. On peut ajuster le comportement de l’OOM killer par processus via `oom_adj`/`oom_score_adj` pour protéger certains services critiques de la terminaison. Une autre astuce concerne le cache disque : dans des scénarios où le système utilise beaucoup de cache inutilement et que les applis manquent de RAM, un cron job qui fait `sync; echo 3 > /proc/sys/vm/drop_caches` de temps en temps peut libérer les caches (au prix de performances disque légèrement réduites ensuite). Cette approche est cependant à utiliser prudemment et en dernier recours. Enfin, si vraiment la mémoire reste un goulot même après optimisation, il faudra envisager une **extension de la RAM** physique, car aucune optimisation ne remplace des ressources insuffisantes.

- **Optimisation des performances des processus :** Linux offre des mécanismes pour ajuster la priorité des processus, tant pour le CPU que pour les IO. La commande `nice` (et `renice`) permet d’assigner une priorité CPU différente à un processus ([Nice (et renice) : la priorité des processus sous Linux | Commandes et Système | IT-Connect](https://www.it-connect.fr/nice-et-renice-la-priorite-des-processus-sous-linux/#:~:text=C%E2%80%99est%20la%20commande%20nice%20qui,20%20%C3%A0%20%2B19)) ([Nice (et renice) : la priorité des processus sous Linux | Commandes et Système | IT-Connect](https://www.it-connect.fr/nice-et-renice-la-priorite-des-processus-sous-linux/#:~:text=suffit%20de%20placer%20sa%20commande,20%20%C3%A0%20%2B19)). Par défaut, tous les processus utilisateurs démarrent avec une valeur de nice de 0. En augmentant la valeur (jusqu’à +19), on **diminue la priorité** du processus, ce qui est utile pour des tâches de fond qui ne doivent pas impacter les tâches interactives. À l’inverse, seules les super-utilisateurs peuvent attribuer des valeurs négatives (jusqu’à -20) pour augmenter la priorité d’un processus critique. Par exemple, si un job de calcul intensif tourne en arrière-plan, le lancer avec `nice -n 19 ./ma_tache` permettra de libérer du temps CPU aux autres programmes. Pour les **entrées/sorties**, la commande équivalente est `ionice` : elle gère la priorité des accès disque. *Ionice* agit au niveau des classes d’ordonnancement IO (temps réel, best-effort par défaut, ou idle). Concrètement, `ionice` fait pour les IO ce que `nice` fait pour le CPU ([Nice (et renice) : la priorité des processus sous Linux | Commandes et Système | IT-Connect](https://www.it-connect.fr/nice-et-renice-la-priorite-des-processus-sous-linux/#:~:text=ionice%20fait%20au%20niveau%20stockage%2C,nice%20fait%20au%20niveau%20CPU)). Un processus mis en classe idle via `ionice -c3` n’effectuera des IO disque que lorsque le disque est inoccupé par d’autres tâches, ce qui évite qu’une sauvegarde lourde ne bloque par exemple les accès disques d’une base de données en production. De même, un processus crucial comme un collecteur de journaux pourrait être mis en priorité IO plus élevée (ionice -c2 -n0) pour qu’il écrive ses données en priorité. En réglant soigneusement les priorités CPU (`nice`) et IO (`ionice`), on **alloue les ressources selon les besoins** : les tâches de fond ou non urgentes sont ralenties en cas de concurrence, tandis que les tâches critiques conservent des performances. Il faut toutefois rester mesuré : une priorité temps réel mal gérée peut monopoliser une ressource au détriment de tout le reste, donc on évitera les valeurs extrêmes sauf nécessité absolue.

- **Gestion fine de l’ordonnancement des tâches (schedtool, chrt) :** Pour aller plus loin que nice/ionice, Linux permet de fixer des politiques d’ordonnancement spécifiques à certains processus. La commande `chrt` (utilitaire fourni par util-linux) peut démarrer ou modifier un processus avec une politique *temps réel* comme SCHED_FIFO ou SCHED_RR, ou au contraire la politique SCHED_IDLE pour en faire une tâche très basse priorité ([chrt - Manipuler les attributs temps réel d'un processus](https://manpages.ubuntu.com/manpages/bionic/fr/man1/chrt.1.html#:~:text=chrt%20,une%20valeur)). Par exemple, `chrt -f 20 <commande>` lancera le processus en SCHED_FIFO avec une priorité fixe de 20 (sur 99 maximum). C’est utile pour des tâches qui doivent absolument être exécutées sans délai (certaines applications audio/vidéo, ou des processus critiques en industrie). À l’inverse, `chrt -i 0 <commande>` lance en SCHED_IDLE (priorité minimale absolue). **Schedtool** est un outil alternatif qui offre une interface plus conviviale pour ces réglages et permet en plus de définir l’**affinité CPU** facilement ([freequaos/schedtool: query and set CPU scheduling parameters in ...](https://github.com/freequaos/schedtool#:~:text=freequaos%2Fschedtool%3A%20query%20and%20set%20CPU,certain%20CPUs%20on%20SMP%2F)). On peut, par exemple, isoler un processus sur un ensemble de cœurs dédiés (évitant qu’il n’interfère avec d’autres). L’usage de ces outils d’ordonnancement avancé doit être réservé à des cas précis car une tâche en temps réel mal configurée peut affamer le CPU et bloquer le système. En environnement serveur classique, on les utilise rarement, sauf pour accorder à une tâche une priorité très haute ponctuellement. Ils démontrent toutefois la flexibilité de Linux pour s’adapter aux besoins : il est possible de **personnaliser la stratégie d’exécution** des processus pour correspondre exactement aux exigences (par exemple, garantir qu’un process A passe toujours avant un process B). Dans la plupart des cas d’optimisation, on se contente de jouer sur nice/ionice, mais chrt/schedtool peuvent résoudre des problèmes de latence extrême dans certaines applications en donnant un contrôle total sur l’ordonnancement.

En appliquant ces optimisations, on vise à **éliminer le ou les goulets d’étranglement** identifiés. Il est conseillé de changer un paramètre à la fois et de mesurer l’effet (à l’aide des outils de la section 3) pour s’assurer que l’ajustement améliore bien la situation. Par exemple, si on réduit `swappiness`, on vérifiera ensuite que le swap diminue et que la latence perçue s’améliore. Si on renice un processus, on observera que le CPU est redistribué aux autres tâches. Cette démarche itérative de *tuning* permet d’atteindre un équilibre où chaque ressource est utilisée de manière optimale sans saturer, améliorant ainsi la performance globale du système Debian.

## 5. Introduction à la supervision centralisée avec Prometheus

Afin d’aller plus loin dans le suivi des performances, il est recommandé de mettre en place une **supervision centralisée** qui collecte les métriques du système en continu et offre des visualisations et alertes. Un des outils phares du monde open-source pour ce besoin est **Prometheus**, souvent couplé à **Grafana** pour la partie tableau de bord. Cette stack de supervision s’installe très bien sur Debian.

### Installation de Prometheus et Node Exporter

Prometheus fonctionne selon un modèle *pull* : il va régulièrement interroger des **exporters** qui exposent des métriques. Pour superviser un système Debian, on déploie l’**exporter de nœud** (*node_exporter*) sur la machine à surveiller. Cet exporter, une fois lancé, collecte un large éventail de métriques matérielles et noyau – utilisation CPU par mode, mémoire utilisée/libre, statistiques disque (IO/s, usage, temps d’attente), trafic réseau, nombre de processus, etc. – et les expose via HTTP. Prometheus va scruter cet endpoint périodiquement (par défaut chaque 15s ou 30s) et stocker les données temporelles. Sur Debian, Prometheus et node_exporter sont disponibles via APT : par exemple, on peut installer les paquets `prometheus` et `prometheus-node-exporter` (qui déposeront des fichiers de service systemd). Une fois `prometheus-node-exporter` activé, il écoute en général sur le port 9100. En visitant `http://<serveur>:9100/metrics`, on peut voir toutes les métriques brutes au format texte. Prometheus, quant à lui, écoute sur le port 9090 par défaut. Il faut configurer Prometheus pour qu’il scrute le node_exporter. Pour cela, on édite `/etc/prometheus/prometheus.yml` et on ajoute un *job* de scrape pointant vers l’IP et le port 9100 du serveur (ou localhost:9100 si Prometheus est sur la même machine) ([How to install and configure Grafana and Prometheus on Debian 12 – D4D Blog](https://www.d4d.lt/how-to-install-and-configure-grafana-and-prometheus-on-debian-12#:~:text=scrape_configs%3A%20...%20,targets%3A%20%5B%22remote_addr%3A9100)) ([How to install and configure Grafana and Prometheus on Debian 12 – D4D Blog](https://www.d4d.lt/how-to-install-and-configure-grafana-and-prometheus-on-debian-12#:~:text=nano%20%2Fetc%2Fprometheus%2Fprometheus)). On peut définir un intervalle de collecte (ex. 10s) selon la granularité souhaitée. Une fois Prometheus démarré, on vérifiera via l’interface web (http://<serveur>:9090, section **Status > Targets**) que le target node_exporter apparaît comme *UP* ([How to install and configure Grafana and Prometheus on Debian 12 – D4D Blog](https://www.d4d.lt/how-to-install-and-configure-grafana-and-prometheus-on-debian-12#:~:text=systemctl%20restart%20prometheus)). Désormais, Prometheus stocke en base temporelle toutes les métriques système. Par exemple, la métrique `node_cpu_seconds_total{mode="system"}` cumule les secondes passées en mode kernel, `node_network_receive_bytes_total` accumule les octets reçus sur chaque interface réseau, etc. Ces données seront utilisées pour le graphing et l’alerting.

### Tableaux de bord Grafana pour visualiser les performances

**Grafana** est une plateforme de visualisation de métriques qui s’intègre parfaitement avec Prometheus. Une fois Prometheus en place, on installe Grafana (via le paquet `grafana` ou l’installation depuis le repo officiel) sur Debian. Grafana possède sa propre interface web (par défaut sur le port 3000). Après installation, on se connecte à Grafana et on ajoute Prometheus comme **source de données** (URL du Prometheus local, généralement `http://localhost:9090`) ([How to install and configure Grafana and Prometheus on Debian 12 – D4D Blog](https://www.d4d.lt/how-to-install-and-configure-grafana-and-prometheus-on-debian-12#:~:text=3,and%20select%20Add%20New%20Connection)). Ensuite, on peut créer des **dashboards** personnalisés ou importer des tableaux de bord prédéfinis de la communauté. Pour superviser un système Linux classique, Grafana Labs fournit un tableau de bord appelé *Node Exporter Full* (ID 1860 sur le catalogue Grafana) qui affiche une multitude d’indicateurs du node_exporter : utilisation CPU (par cœur, par mode), utilisation mémoire (RAM utilisée, cache, swap), charges disque (IOPS, latence, utilisation par disque), trafic réseau, etc., le tout avec des graphiques historiques. On peut soit l’importer directement en entrant son ID dans Grafana, soit créer un dashboard plus simple. Par exemple, on pourra configurer un graphique montrant la **charge CPU** : on utilise la requête PromQL `avg(rate(node_cpu_seconds_total{mode="user"}[5m]))` pour la charge user sur 5 minutes, et de même pour system, iowait, etc., et on les empile en aires. De même, un graphique de **mémoire** montrera `node_memory_MemAvailable` (mémoire disponible) vs `node_memory_MemTotal` pour voir l’utilisation, ou un graphique de **disque** utilisera la différence de `node_disk_read_bytes_total` pour calculer un débit de lecture. Grafana permet également des jauges, par exemple pour la température CPU si le node_exporter la fournit (`node_temperature_core...`). Bref, la création de **tableaux de bord** est très flexible. L’important est de sélectionner les métriques clés à surveiller : CPU, RAM, disque, réseau, et éventuellement des métriques plus haut niveau comme le taux de context switch (`node_context_switches_total`), le load average (`node_load1` etc.), le nombre de processus (`node_processes`), etc. En quelques clics, on obtient ainsi une vue d’ensemble des performances du serveur, visualisable à distance via Grafana.

 ([Node exporter simple | Grafana Labs
](https://grafana.com/grafana/dashboards/10372-node-exporter-simple/)) *Exemple de tableau de bord Grafana simple pour un serveur Debian (via Node Exporter). On y voit l’utilisation CPU répartie par mode (user, system, iowait, etc.), l’usage de la mémoire (utilisée, cache, libre), le trafic réseau par interface, et l’espace disque utilisé. Chaque graphique affiche l’évolution sur les dernières heures, ce qui permet de repérer facilement les tendances et pics d’activité.*

Au-delà de la visualisation, Grafana permet aussi la mise en place d’**alertes** basées sur les métriques Prometheus. Typiquement, on définit dans Prometheus des règles d’alerte (dans un fichier de règles ou via Alertmanager) pour détecter des conditions anormales : par exemple, *CPU usage > 90% pendant 5 minutes*, *mémoire disponible < 500 Mo*, *taux d’erreurs disque*, etc. Une règle d’alerte Prometheus se décrit avec une expression et une durée : par ex., pour le CPU on pourrait avoir: 

```yaml
- alert: HighCpuLoad
  expr: avg(rate(node_cpu_seconds_total{mode!="idle"}[5m])) > 0.9
  for: 5m
  labels:
    severity: warning
  annotations:
    description: "CPU util prolongé >90%"
``` 

Ce genre de règle déclenche une alerte si sur 5 minutes le CPU moyen dépasse 90% ([Alerting rules | Prometheus](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#:~:text=team%3A%20myteam%20rules%3A%20,5m%20labels%3A%20severity%3A%20page%20annotations)). Prometheus évalue les règles toutes les minutes et marque l’alerte comme *firing* quand la condition est vraie. Grafana de son côté peut être configuré (ou plus souvent on utilise Alertmanager) pour envoyer des notifications lorsque l’alerte est déclenchée – typiquement par email, SMS ou sur Slack. L’interface Grafana permet aussi de définir des alertes directement depuis un panneau de graphique : on trace une ligne de seuil et on demande à Grafana de notifier si la courbe la dépasse pendant un certain temps. Cependant, l’approche moderne est d’utiliser l’Alerting de Prometheus avec l’**Alertmanager** qui gère l’envoi et le routage des alertes. On peut par exemple configurer Alertmanager pour envoyer une alerte “CPU élevé sur serveur X” lorsque la règle HighCpuLoad fire. De même, on définira des seuils pour le taux d’utilisation du swap, la saturation d’un disque (par ex. disque rempli à 90%), ou encore l’absence de cœur idle (ce qui serait un signe de surcharge CPU constante). L’important est de calibrer des alertes **pertinentes** afin d’être prévenu en cas d’anomalie de performance avant que celle-ci n’affecte gravement les utilisateurs. Par exemple, une alerte *Warning* à 85% CPU puis *Critical* à 95% CPU, ou une alerte sur un load average > 2× le nombre de cœurs pendant 15 minutes, etc. En associant ces alertes à Grafana/Prometheus, on se dote d’un système de **supervision proactif** : non seulement on peut consulter à tout moment l’état passé et présent du système via les dashboards, mais on reçoit aussi des notifications automatiques si une métrique sort des plages normales.

En conclusion, la mise en place de Prometheus et Grafana sur Debian apporte une **visibilité continue** sur les performances. On passe d’une analyse ponctuelle en ligne de commande à une surveillance en continu avec historisation. Les métriques collectées (CPU, RAM, disque, réseau, etc.) peuvent être explorées a posteriori pour comprendre un incident survenu dans la nuit, créer des **tableaux de bord** pour visualiser l’impact des changements (par exemple, voir la baisse de l’usage swap après un réglage de swappiness), et définir des **alertes** pour réagir rapidement en cas de nouvelle dégradation. Cette supervision centralisée s’inscrit donc comme le prolongement naturel des bonnes pratiques de contrôle de performance sur Debian, permettant d’**anticiper les problèmes** et d’optimiser en permanence le système. 

